<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Spark DataFrame操作 | zhang35's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark DataFrame操作</h1><a id="logo" href="/.">zhang35's Blog</a><p class="description">日积月累</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 主页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Spark DataFrame操作</h1><div class="post-meta">2020-09-30<span> | </span><span class="category"><a href="/categories/spark/">spark</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3,993</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 25</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_time_int</span><span class="params">(d, t)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> time.mktime(time.strptime(d + <span class="string">','</span> + t, time_format))</span><br><span class="line">indf = inputRDD(inputTableName)\</span><br><span class="line">        .withColumnRenamed(lonCol, <span class="string">'longitude'</span>)\</span><br><span class="line">        .withColumnRenamed(latCol, <span class="string">'latitude'</span>)\</span><br><span class="line">        .withColumnRenamed(dateCol, <span class="string">'date'</span>)\</span><br><span class="line">        .withColumnRenamed(timeCol, <span class="string">'time'</span>)</span><br><span class="line"><span class="comment"># 将日期+时间统一为秒数，增加新列"timeInt"存储</span></span><br><span class="line">indf = indf.withColumn(<span class="string">"timeInt"</span>, functions.UserDefinedFunction(calc_time_int)(indf.date, indf.time))</span><br><span class="line"><span class="comment"># 按照"timeInt"排序</span></span><br><span class="line">indf = indf.withColumn(<span class="string">"timeInt"</span>, indf[<span class="string">"timeInt"</span>].cast(<span class="string">"double"</span>))</span><br><span class="line">points = indf.orderBy(<span class="string">'timeInt'</span>).collect()</span><br></pre></td></tr></table></figure>
<p><a href="https://towardsdatascience.com/the-most-complete-guide-to-pyspark-dataframes-2702c343b2e8#35c2" target="_blank" rel="noopener">https://towardsdatascience.com/the-most-complete-guide-to-pyspark-dataframes-2702c343b2e8#35c2</a>)</p>
<h1 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h1><p>I am working on installing Spark on Ubuntu 18.04, but the steps should remain the same for MAC too. I am assuming that you already have Anaconda and Python3 installed. After that, you can just go through these steps:</p>
<ol>
<li>Download the Spark Binary from Apache Spark <a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">Website</a>. And click on the Download Spark link to download Spark.</li>
</ol>
<p><img src="https://miro.medium.com/max/60/1*-xm3faSPHpjQCu8RuHD8bA.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/1175/1*-xm3faSPHpjQCu8RuHD8bA.png" alt="Image for post"></p>
<p>\2. Once you have downloaded the above file, you can start with unzipping the file in your home directory. Just Open up the terminal and put these commands in.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ~</span><br><span class="line">cp Downloads/spark-2.4.5-bin-hadoop2.7.tgz ~</span><br><span class="line">tar -zxvf spark-2.4.5-bin-hadoop2.7.tgz</span><br></pre></td></tr></table></figure>
<p>\3. Check your Java Version. As of version 2.4 Spark works with Java 8. You can check your Java Version using the command <code>java -version</code> on the terminal window.</p>
<p>I had Java 11 in my machine, so I had to run the following commands on my terminal to install and change default Java to Java 8:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install openjdk-8-jdk</span><br><span class="line">sudo update-alternatives --config java</span><br></pre></td></tr></table></figure>
<p>You will need to manually select the Java version 8 by typing the selection number.</p>
<p><img src="https://miro.medium.com/max/60/1*YTG1Hpovti8bcb3PFNPfBw.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/1214/1*YTG1Hpovti8bcb3PFNPfBw.png" alt="Image for post"></p>
<p>Rechecking Java version should give something like:</p>
<p><img src="https://miro.medium.com/max/60/1*_m6cmKerEl4K1YyVY81kcQ.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/1203/1*_m6cmKerEl4K1YyVY81kcQ.png" alt="Image for post"></p>
<p>\4. Edit your <code>~/.bashrc</code> file and add the following lines at the end of the file:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">function pysparknb () </span><br><span class="line">&#123;</span><br><span class="line">#Spark path</span><br><span class="line">SPARK_PATH=~/spark-2.4.5-bin-hadoop2.7export PYSPARK_DRIVER_PYTHON=&quot;jupyter&quot;</span><br><span class="line">export PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook&quot;# For pyarrow 0.15 users, you have to add the line below or you will get an error while using pandas_udf </span><br><span class="line">export ARROW_PRE_0_15_IPC_FORMAT=1# Change the local[10] to local[numCores in your machine]</span><br><span class="line">$SPARK_PATH/bin/pyspark --master local[10]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>\5. Source <code>~/.bashrc</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>\6. Run the <code>pysparknb</code> function in the terminal and you will be able to access the notebook. You will be able to open a new notebook as well as the <code>sparkcontext</code> will be loaded automatically.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pysparknb</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*2PO5uExGdBF2T2VAMBSxVw.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/1154/1*2PO5uExGdBF2T2VAMBSxVw.png" alt="Image for post"></p>
<h1 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h1><p>With the installation out of the way, we can move to the more interesting part of this post. I will be working with the <a href="https://www.kaggle.com/kimjihoo/coronavirusdataset" target="_blank" rel="noopener">Data Science for COVID-19 in South Korea</a>, which is one of the most detailed datasets on the internet for COVID.</p>
<p>Please note that I will be using this dataset to showcase some of the most useful functionalities of Spark, but this should not be in any way considered a data exploration exercise for this amazing dataset.</p>
<p><img src="https://miro.medium.com/max/60/0*RVqtYcbfYPdKuU2L.PNG?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/1174/0*RVqtYcbfYPdKuU2L.PNG" alt="Image for post"></p>
<p>Source: <a href="https://www.kaggle.com/kimjihoo/coronavirusdataset?select=Case.csv" target="_blank" rel="noopener">Kaggle</a></p>
<p>I will mainly work with the following three tables only in this post:</p>
<ul>
<li>Cases</li>
<li>Region</li>
<li>TimeProvince</li>
</ul>
<p><strong><em>You can find all the code at the\</em></strong> <a href="https://github.com/MLWhiz/data_science_blogs/tree/master/sparkdf" target="_blank" rel="noopener"><strong><em>GitHub\</em></strong></a> <strong><em>repository.\</em></strong></p>
<h1 id="1-Basic-Functions"><a href="#1-Basic-Functions" class="headerlink" title="1. Basic Functions"></a>1. Basic Functions</h1><h2 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h2><p>We can start by loading the files in our dataset using the spark.read.load command. This command reads parquet files, which is the default file format for spark, but you can add the parameter <code>format</code> to read .csv files using it.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cases = spark.read.load(&quot;/home/rahul/projects/sparkdf/coronavirusdataset/Case.csv&quot;,format=&quot;csv&quot;, sep=&quot;,&quot;, inferSchema=&quot;true&quot;, header=&quot;true&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="See-a-few-rows-in-the-file"><a href="#See-a-few-rows-in-the-file" class="headerlink" title="See a few rows in the file"></a>See a few rows in the file</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cases.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*ktmyd5NDOpjNlgGvaZ8gaA.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/989/1*ktmyd5NDOpjNlgGvaZ8gaA.png" alt="Image for post"></p>
<p>This file contains the cases grouped by way of the infection spread. This might have helped in the rigorous tracking of Corona Cases in South Korea.</p>
<p>The way this file looks is great right now, but sometimes as we increase the number of columns, the formatting becomes not too great. I have noticed that the following trick helps in displaying in pandas format in my Jupyter Notebook. The <code>.toPandas()</code> function converts a spark dataframe into a pandas Dataframe which is easier to show.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cases.limit(10).toPandas()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*FsFK46Nn5A5bqGClBisNyw.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/883/1*FsFK46Nn5A5bqGClBisNyw.png" alt="Image for post"></p>
<h2 id="Change-Column-Names"><a href="#Change-Column-Names" class="headerlink" title="Change Column Names"></a>Change Column Names</h2><p>Sometimes we would like to change the name of columns in our Spark Dataframes. We can do this simply using the below command to change a single column:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cases = cases.withColumnRenamed(&quot;infection_case&quot;,&quot;infection_source&quot;)</span><br></pre></td></tr></table></figure>
<p>Or for all columns:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cases = cases.toDF(*[&apos;case_id&apos;, &apos;province&apos;, &apos;city&apos;, &apos;group&apos;, &apos;infection_case&apos;, &apos;confirmed&apos;,</span><br><span class="line">       &apos;latitude&apos;, &apos;longitude&apos;])</span><br></pre></td></tr></table></figure>
<h2 id="Select-Columns"><a href="#Select-Columns" class="headerlink" title="Select Columns"></a>Select Columns</h2><p>We can select a subset of columns using the <code>select</code> keyword.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cases = cases.select(&apos;province&apos;,&apos;city&apos;,&apos;infection_case&apos;,&apos;confirmed&apos;)</span><br><span class="line">cases.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*QKcHoVIlK2GXcH-nBl4jbg.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/892/1*QKcHoVIlK2GXcH-nBl4jbg.png" alt="Image for post"></p>
<h2 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h2><p>We can sort by the number of confirmed cases. Here note that the <code>cases</code> data frame will not change after performing this command as we don’t assign it to any variable.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cases.sort(&quot;confirmed&quot;).show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*8qGT0xInxQIWty9t0t2XFQ.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/895/1*8qGT0xInxQIWty9t0t2XFQ.png" alt="Image for post"></p>
<p>But that is inverted. We want to see the most cases at the top. We can do this using the <code>F.desc</code> function:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># descending Sort</span><br><span class="line">from pyspark.sql import functions as F</span><br><span class="line">cases.sort(F.desc(&quot;confirmed&quot;)).show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*R6aM7HxbFncerkpwWquZhQ.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/898/1*R6aM7HxbFncerkpwWquZhQ.png" alt="Image for post"></p>
<p>We can see the most cases in a logical area in South Korea originated from <code>Shincheonji Church</code>.</p>
<h2 id="Cast"><a href="#Cast" class="headerlink" title="Cast"></a>Cast</h2><p>Though we don’t face it in this dataset, there might be scenarios where Pyspark reads a double as integer or string, In such cases, you can use the cast function to convert types.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql.types import DoubleType, IntegerType, StringTypecases = cases.withColumn(&apos;confirmed&apos;, F.col(&apos;confirmed&apos;).cast(IntegerType()))cases = cases.withColumn(&apos;city&apos;, F.col(&apos;city&apos;).cast(StringType()))</span><br></pre></td></tr></table></figure>
<h2 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h2><p>We can filter a data frame using multiple conditions using AND(&amp;), OR(|) and NOT(~) conditions. For example, we may want to find out all the different infection_case in Daegu Province with more than 10 confirmed cases.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cases.filter((cases.confirmed&gt;10) &amp; (cases.province==&apos;Daegu&apos;)).show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*hgJ2QZuDCmNpTg6ust-FVA.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/896/1*hgJ2QZuDCmNpTg6ust-FVA.png" alt="Image for post"></p>
<h2 id="GroupBy"><a href="#GroupBy" class="headerlink" title="GroupBy"></a>GroupBy</h2><p>We can use <code>groupBy</code> function with a spark DataFrame too. Pretty much same as the pandas <code>groupBy</code> with the exception that you will need to import <code>pyspark.sql.functions</code>. <a href="https://people.eecs.berkeley.edu/~jegonzal/pyspark/pyspark.sql.html#module-pyspark.sql.functions" target="_blank" rel="noopener">Here</a> is the list of functions you can use with this function module.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import functions as Fcases.groupBy([&quot;province&quot;,&quot;city&quot;]).agg(F.sum(&quot;confirmed&quot;) ,F.max(&quot;confirmed&quot;)).show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*cp3SUlUCAKXtqIg-WmBfUQ.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/896/1*cp3SUlUCAKXtqIg-WmBfUQ.png" alt="Image for post"></p>
<p>If you don’t like the new column names, you can use the <code>alias</code> keyword to rename columns in the <code>agg</code> command itself.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cases.groupBy([&quot;province&quot;,&quot;city&quot;]).agg(</span><br><span class="line">    F.sum(&quot;confirmed&quot;).alias(&quot;TotalConfirmed&quot;),\</span><br><span class="line">    F.max(&quot;confirmed&quot;).alias(&quot;MaxFromOneConfirmedCase&quot;)\</span><br><span class="line">    ).show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*FY498DJ6Zq_85ISVh0Y2Bw.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/894/1*FY498DJ6Zq_85ISVh0Y2Bw.png" alt="Image for post"></p>
<h2 id="Joins"><a href="#Joins" class="headerlink" title="Joins"></a>Joins</h2><p>To Start with Joins we will need to introduce one more CSV file. We will go with the region file which contains region information such as elementary_school_count, elderly_population_ratio, etc.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">regions = spark.read.load(&quot;/home/rahul/projects/sparkdf/coronavirusdataset/Region.csv&quot;,format=&quot;csv&quot;, sep=&quot;,&quot;, inferSchema=&quot;true&quot;, header=&quot;true&quot;)</span><br><span class="line">regions.limit(10).toPandas()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*hUS5-k5chLXW_9y-WYiQwg.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/1149/1*hUS5-k5chLXW_9y-WYiQwg.png" alt="Image for post"></p>
<p>We want to get this information in our cases file by joining the two DataFrames. We can do this by using:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cases = cases.join(regions, [&apos;province&apos;,&apos;city&apos;],how=&apos;left&apos;)</span><br><span class="line">cases.limit(10).toPandas()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*Ms2nehD-DeuKbL28v89tDw.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/1339/1*Ms2nehD-DeuKbL28v89tDw.png" alt="Image for post"></p>
<h1 id="2-Broadcast-Map-Side-Joins"><a href="#2-Broadcast-Map-Side-Joins" class="headerlink" title="2. Broadcast/Map Side Joins"></a>2. Broadcast/Map Side Joins</h1><p>Sometimes you might face a scenario where you need to join a very big table(~1B Rows) with a very small table(~100–200 rows). The scenario might also involve increasing the size of your database like in the example below.</p>
<p><img src="https://miro.medium.com/max/60/1*8XzZhDG53AP5ejj7ULkO7A.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/706/1*8XzZhDG53AP5ejj7ULkO7A.png" alt="Image for post"></p>
<p>Such sort of operations is aplenty in Spark where you might want to apply multiple operations to a particular key. But assuming that the data for each key in the Big table is large, it will involve a lot of data movement. And sometimes so much that the application itself breaks. A small optimization then you can do when joining on such big tables(assuming the other table is small) is to broadcast the small table to each machine/node when you perform a join. You can do this easily using the broadcast keyword. This has been a lifesaver many times with Spark when everything else fails.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql.functions import broadcast</span><br><span class="line">cases = cases.join(broadcast(regions), [&apos;province&apos;,&apos;city&apos;],how=&apos;left&apos;)</span><br></pre></td></tr></table></figure>
<h1 id="3-Use-SQL-with-DataFrames"><a href="#3-Use-SQL-with-DataFrames" class="headerlink" title="3. Use SQL with DataFrames"></a>3. Use SQL with DataFrames</h1><p>If you want, you can also use SQL with data frames. Let us try to run some SQL on the cases table.</p>
<p>We first register the cases dataframe to a temporary table cases_table on which we can run SQL operations. As you can see, the result of the SQL select statement is again a Spark Dataframe.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cases.registerTempTable(&apos;cases_table&apos;)</span><br><span class="line">newDF = sqlContext.sql(&apos;select * from cases_table where confirmed&gt;100&apos;)</span><br><span class="line">newDF.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*mE7MvOsWN_Zy8Pg1B9arMg.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/895/1*mE7MvOsWN_Zy8Pg1B9arMg.png" alt="Image for post"></p>
<p>I have shown a minimal example above, but you can use pretty much complex SQL queries involving GROUP BY, HAVING, AND ORDER BY clauses as well as aliases in the above query.</p>
<h1 id="4-Create-New-Columns"><a href="#4-Create-New-Columns" class="headerlink" title="4. Create New Columns"></a>4. Create New Columns</h1><p>There are many ways that you can use to create a column in a PySpark Dataframe. I will try to show the most usable of them.</p>
<h2 id="Using-Spark-Native-Functions"><a href="#Using-Spark-Native-Functions" class="headerlink" title="Using Spark Native Functions"></a>Using Spark Native Functions</h2><p>The most pysparkish way to create a new column in a PySpark DataFrame is by using built-in functions. This is the most performant programmatical way to create a new column, so this is the first place I go whenever I want to do some column manipulation.</p>
<p>We can use <code>.withcolumn</code> along with PySpark SQL functions to create a new column. In essence, you can find String functions, Date functions, and Math functions already implemented using Spark functions. Our first function, the <code>F.col</code> function gives us access to the column. So if we wanted to add 100 to a column, we could use <code>F.col</code> as:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import pyspark.sql.functions as F</span><br><span class="line">casesWithNewConfirmed = cases.withColumn(&quot;NewConfirmed&quot;, 100 + F.col(&quot;confirmed&quot;))casesWithNewConfirmed.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*iTG_N0x4KRdCQP5avzhBYA.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/895/1*iTG_N0x4KRdCQP5avzhBYA.png" alt="Image for post"></p>
<p>We can also use math functions like <code>F.exp</code> function:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">casesWithExpConfirmed = cases.withColumn(&quot;ExpConfirmed&quot;, F.exp(&quot;confirmed&quot;))casesWithExpConfirmed.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*JaIautuqNZT0me3uVKTZJg.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/892/1*JaIautuqNZT0me3uVKTZJg.png" alt="Image for post"></p>
<p>There are a lot of other functions provided in this module, which are enough for most simple use cases. You can check out the functions list <a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions" target="_blank" rel="noopener">here</a>.</p>
<h2 id="Using-Spark-UDFs"><a href="#Using-Spark-UDFs" class="headerlink" title="Using Spark UDFs"></a>Using Spark UDFs</h2><p>Sometimes we want to do complicated things to a column or multiple columns. This could be thought of as a map operation on a PySpark Dataframe to a single column or multiple columns. While Spark SQL functions do solve many use cases when it comes to column creation, I use Spark UDF whenever I need more matured Python functionality.</p>
<p>To use Spark UDFs, we need to use the <code>F.udf</code> function to convert a regular python function to a Spark UDF. We also need to specify the return type of the function. In this example the return type is <code>StringType()</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import pyspark.sql.functions as F</span><br><span class="line">from pyspark.sql.types import *</span><br><span class="line">def casesHighLow(confirmed):</span><br><span class="line">    if confirmed &lt; 50: </span><br><span class="line">        return &apos;low&apos;</span><br><span class="line">    else:</span><br><span class="line">        return &apos;high&apos;</span><br><span class="line">    </span><br><span class="line">#convert to a UDF Function by passing in the function and return type of function</span><br><span class="line">casesHighLowUDF = F.udf(casesHighLow, StringType())CasesWithHighLow = cases.withColumn(&quot;HighLow&quot;, casesHighLowUDF(&quot;confirmed&quot;))</span><br><span class="line">CasesWithHighLow.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*RcawXm0DOYunfIxnXb7voQ.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/893/1*RcawXm0DOYunfIxnXb7voQ.png" alt="Image for post"></p>
<h2 id="Using-RDDs"><a href="#Using-RDDs" class="headerlink" title="Using RDDs"></a>Using RDDs</h2><p>This might seem a little odd, but sometimes both the spark UDFs and SQL functions are not enough for a particular use-case. I have observed the RDDs being much more performant in some use-cases in real life. You might want to utilize the better partitioning that you get with spark RDDs. Or you may want to use group functions in Spark RDDs.</p>
<p>Whatever the case be, I find this way of using RDD to create new columns pretty useful for people who have experience working with RDDs that is the basic building block in the Spark ecosystem. Don’t worry much if you don’t understand it. It is just here for completion.</p>
<p><strong><em>The process below makes use of the functionality to convert between\</em></strong> <code>***Row\***</code> <strong><em>and\</em></strong> <code>***pythondict\***</code> <strong><em>objects.\</em></strong> We convert a row object to a dictionary. Work with the dictionary as we are used to and convert that dictionary back to row again. This might come in handy in a lot of situations.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line">from pyspark.sql import Rowdef rowwise_function(row):</span><br><span class="line">    # convert row to python dictionary:</span><br><span class="line">    row_dict = row.asDict()</span><br><span class="line">    # Add a new key in the dictionary with the new column name and value.</span><br><span class="line">    # This might be a big complex function.</span><br><span class="line">    row_dict[&apos;expConfirmed&apos;] = float(np.exp(row_dict[&apos;confirmed&apos;]))</span><br><span class="line">    # convert dict to row back again:</span><br><span class="line">    newrow = Row(**row_dict)</span><br><span class="line">    # return new row</span><br><span class="line">    return newrow# convert cases dataframe to RDD</span><br><span class="line">cases_rdd = cases.rdd# apply our function to RDD</span><br><span class="line">cases_rdd_new = cases_rdd.map(lambda row: rowwise_function(row))# Convert RDD Back to DataFrame</span><br><span class="line">casesNewDf = sqlContext.createDataFrame(cases_rdd_new)casesNewDf.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*t73b854I0Hdlks5wbQQx_Q.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/893/1*t73b854I0Hdlks5wbQQx_Q.png" alt="Image for post"></p>
<h2 id="Using-Pandas-UDF"><a href="#Using-Pandas-UDF" class="headerlink" title="Using Pandas UDF"></a>Using Pandas UDF</h2><p>This functionality was introduced in the Spark version 2.3.1. And this allows you to use pandas functionality with Spark. I generally use it when I have to run a groupBy operation on a Spark dataframe or whenever I need to create rolling features and want to use Pandas rolling functions/window functions rather than Spark window functions which we will go through later in this post.</p>
<p>The way we use it is by using the <code>F.pandas_udf</code> decorator. <strong><em>We assume here that the input to the function will be a pandas data frame.\</em></strong> And we need to return a pandas dataframe in turn from this function.</p>
<p>The only complexity here is that we have to provide a schema for the output Dataframe. We can use the original schema of a dataframe to create the outSchema.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cases.printSchema()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*an_O0MpNsOqVMioU1ne67g.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/893/1*an_O0MpNsOqVMioU1ne67g.png" alt="Image for post"></p>
<p>Here I am using Pandas UDF to get normalized confirmed cases grouped by infection_case. The main advantage here is that I get to work with pandas dataframes in Spark.</p>
<iframe src="https://towardsdatascience.com/media/14e602004a77fc390f9fd960237da04f" allowfullscreen frameborder="0" height="633" width="680" title="pandas_udf.py" class="t u v gg aj" scrolling="auto" style="box-sizing: inherit; position: absolute; top: 0px; left: 0px; width: 680px; height: 633px;"></iframe>

<p><img src="https://miro.medium.com/max/60/1*iUOEcEXVNsYtabSnhXXgwA.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/819/1*iUOEcEXVNsYtabSnhXXgwA.png" alt="Image for post"></p>
<h1 id="5-Spark-Window-Functions"><a href="#5-Spark-Window-Functions" class="headerlink" title="5. Spark Window Functions"></a>5. Spark Window Functions</h1><p>Window functions may make a whole blog post in itself. Here I will talk about some of the most important window functions available in spark.</p>
<p>For this, I will also use one more data CSV, which has dates present as that will help with understanding Window functions much better. I will use the TimeProvince dataframe which contains daily case information for each province.</p>
<p><img src="https://miro.medium.com/max/60/1*dAennXpCnDel5e8a2aU16A.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/896/1*dAennXpCnDel5e8a2aU16A.png" alt="Image for post"></p>
<h2 id="Ranking"><a href="#Ranking" class="headerlink" title="Ranking"></a>Ranking</h2><p>You can get rank as well as dense_rank on a group using this function. For example, you may want to have a column in your cases table that provides the rank of infection_case based on the number of infection_case in a province. We can do this by:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql.window import WindowwindowSpec = Window().partitionBy([&apos;province&apos;]).orderBy(F.desc(&apos;confirmed&apos;))cases.withColumn(&quot;rank&quot;,F.rank().over(windowSpec)).show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*k0OR_AL_LMq0SKUpHdD7KQ.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/896/1*k0OR_AL_LMq0SKUpHdD7KQ.png" alt="Image for post"></p>
<h2 id="Lag-Variables"><a href="#Lag-Variables" class="headerlink" title="Lag Variables"></a>Lag Variables</h2><p>Sometimes our data science models may need lag based features. For example, a model might have variables like the price last week or sales quantity the previous day. We can create such features using the lag function with window functions. Here I am trying to get the confirmed cases 7 days before. I am filtering to show the results as the first few days of corona cases were zeros. You can see here that the lag_7 day feature is shifted by 7 days.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql.window import Window</span><br><span class="line">windowSpec = Window().partitionBy([&apos;province&apos;]).orderBy(&apos;date&apos;)</span><br><span class="line">timeprovinceWithLag = timeprovince.withColumn(&quot;lag_7&quot;,F.lag(&quot;confirmed&quot;, 7).over(windowSpec))timeprovinceWithLag.filter(timeprovinceWithLag.date&gt;&apos;2020-03-10&apos;).show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*deE34rlGaCf1lQH1iLhCGA.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/800/1*deE34rlGaCf1lQH1iLhCGA.png" alt="Image for post"></p>
<h2 id="Rolling-Aggregations"><a href="#Rolling-Aggregations" class="headerlink" title="Rolling Aggregations"></a>Rolling Aggregations</h2><p>Sometimes it helps to provide rolling averages to our models. For example, we might want to have a rolling 7-day sales sum/mean as a feature for our sales regression model. Let us calculate the rolling mean of confirmed cases for the last 7 days here. This is what a lot of the people are already doing with this dataset to see the real trends.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql.window import WindowwindowSpec = Window().partitionBy([&apos;province&apos;]).orderBy(&apos;date&apos;).rowsBetween(-6,0)timeprovinceWithRoll = timeprovince.withColumn(&quot;roll_7_confirmed&quot;,F.mean(&quot;confirmed&quot;).over(windowSpec))timeprovinceWithRoll.filter(timeprovinceWithLag.date&gt;&apos;2020-03-10&apos;).show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*MBW-cxmevqPkA1vd5fbo2Q.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/797/1*MBW-cxmevqPkA1vd5fbo2Q.png" alt="Image for post"></p>
<p>There are a few things here to understand. First is the<code>**rowsBetween(-6,0)**</code> function that we are using here. This function has a form of <code>rowsBetween(start,end)</code> with both start and end inclusive. Using this we only look at the past 7 days in a particular window including the current_day. Here 0 specifies the current_row and -6 specifies the seventh row previous to current_row. Remember we count starting from 0.</p>
<p>So to get <code>roll_7_confirmed</code> for date <code>2020–03–22</code> we look at the confirmed cases for dates <code>2020–03–22 to 2020–03–16</code> and take their mean.</p>
<p>If we had used <code>**rowsBetween(-7,-1)**</code> we would just have looked at past 7 days of data and not the current_day.</p>
<p>One could also find a use for <code>rowsBetween(Window.unboundedPreceding, Window.currentRow)</code> where we take the rows between the first row in a window and the current_row to get running totals. I am calculating cumulative_confirmed here.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql.window import WindowwindowSpec = Window().partitionBy([&apos;province&apos;]).orderBy(&apos;date&apos;).rowsBetween(Window.unboundedPreceding,Window.currentRow)</span><br><span class="line">timeprovinceWithRoll = timeprovince.withColumn(&quot;cumulative_confirmed&quot;,F.sum(&quot;confirmed&quot;).over(windowSpec))</span><br><span class="line">timeprovinceWithRoll.filter(timeprovinceWithLag.date&gt;&apos;2020-03-10&apos;).show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*-jfjnbRiCpQxOAviCBnlGw.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/792/1*-jfjnbRiCpQxOAviCBnlGw.png" alt="Image for post"></p>
<h1 id="6-Pivot-Dataframes"><a href="#6-Pivot-Dataframes" class="headerlink" title="6. Pivot Dataframes"></a>6. Pivot Dataframes</h1><p>Sometimes we may need to have the dataframe in flat format. This happens frequently in movie data where we may want to show genres as columns instead of rows. We can use pivot to do this. Here I am trying to get one row for each date and getting the province names as columns.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pivotedTimeprovince = timeprovince.groupBy(&apos;date&apos;).pivot(&apos;province&apos;).agg(F.sum(&apos;confirmed&apos;).alias(&apos;confirmed&apos;) , F.sum(&apos;released&apos;).alias(&apos;released&apos;))pivotedTimeprovince.limit(10).toPandas()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*qY3QiI116g794I7diW5IfA.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/803/1*qY3QiI116g794I7diW5IfA.png" alt="Image for post"></p>
<p>One thing to note here is that we need to provide an aggregation always with the pivot function even if the data has a single row for a date.</p>
<h1 id="7-Unpivot-Stack-Dataframes"><a href="#7-Unpivot-Stack-Dataframes" class="headerlink" title="7. Unpivot/Stack Dataframes"></a>7. Unpivot/Stack Dataframes</h1><p>This is just the opposite of the pivot. Given a pivoted dataframe like above, can we go back to the original?</p>
<p>Yes, we can. But the way is not that straightforward. For one we will need to replace <code>-</code> with <code>_</code> in the column names as it interferes with what we are about to do. We can simply rename the columns:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">newColnames = [x.replace(&quot;-&quot;,&quot;_&quot;) for x in pivotedTimeprovince.columns]pivotedTimeprovince = pivotedTimeprovince.toDF(*newColnames)</span><br></pre></td></tr></table></figure>
<p>Now we will need to create an expression which looks like the below:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;stack(34, &apos;Busan_confirmed&apos; , Busan_confirmed,&apos;Busan_released&apos; , Busan_released,&apos;Chungcheongbuk_do_confirmed&apos; ,.</span><br><span class="line">.</span><br><span class="line">.&apos;Seoul_released&apos; , Seoul_released,&apos;Ulsan_confirmed&apos; , Ulsan_confirmed,&apos;Ulsan_released&apos; , Ulsan_released) as (Type,Value)&quot;</span><br></pre></td></tr></table></figure>
<p>The general format is as follows:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;stack(&lt;cnt of columns you want to put in one column&gt;, &apos;firstcolname&apos;, firstcolname , &apos;secondcolname&apos; ,secondcolname ......) as (Type, Value)&quot;</span><br></pre></td></tr></table></figure>
<p>It may seem daunting, but we can create such an expression using our programming skills.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">expression = &quot;&quot;</span><br><span class="line">cnt=0</span><br><span class="line">for column in pivotedTimeprovince.columns:</span><br><span class="line">    if column!=&apos;date&apos;:</span><br><span class="line">        cnt +=1</span><br><span class="line">        expression += f&quot;&apos;&#123;column&#125;&apos; , &#123;column&#125;,&quot;expression = f&quot;stack(&#123;cnt&#125;, &#123;expression[:-1]&#125;) as (Type,Value)&quot;</span><br></pre></td></tr></table></figure>
<p>And we can unpivot using:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unpivotedTimeprovince = pivotedTimeprovince.select(&apos;date&apos;,F.expr(exprs))</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*hu8pEYBD3x-U1UEA3w5Wkw.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/801/1*hu8pEYBD3x-U1UEA3w5Wkw.png" alt="Image for post"></p>
<p>And voila! we have got our dataframe in a vertical format. There are quite a few column creations, filters, and join operations needed to get exactly the same format as before, but I will not get into those.</p>
<h1 id="8-Salting"><a href="#8-Salting" class="headerlink" title="8. Salting"></a>8. Salting</h1><p>Sometimes it might happen that a lot of data goes to a single executor since the same key is assigned for a lot of rows in our data. Salting is another way that helps you to manage data skewness.</p>
<p>So assuming we want to do the sum operation when we have skewed keys. We can start by creating the Salted Key and then doing a double aggregation on that key as the sum of a sum still equals sum. To understand this assume we need the sum of confirmed infection_cases on the cases table and assume that the key infection_cases is skewed. We can do the required operation in two steps.</p>
<p><strong>1. Create a Salting Key</strong></p>
<p>We first create a salting key using a concatenation of infection_case column and a random_number between 0 to 9. In case your key is even more skewed, you can split it in even more than 10 parts.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cases = cases.withColumn(&quot;salt_key&quot;, F.concat(F.col(&quot;infection_case&quot;), F.lit(&quot;_&quot;), F.monotonically_increasing_id() % 10))</span><br></pre></td></tr></table></figure>
<p>This is how the table looks after the operation:</p>
<p><img src="https://miro.medium.com/max/60/1*lqeOANtoNPprEAxcpB56NQ.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/990/1*lqeOANtoNPprEAxcpB56NQ.png" alt="Image for post"></p>
<p><strong>2. First Groupby on salt key</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cases_temp = cases.groupBy([&quot;infection_case&quot;,&quot;salt_key&quot;]).agg(F.sum(&quot;confirmed&quot;)).show()</span><br></pre></td></tr></table></figure>
<p><img src="https://miro.medium.com/max/60/1*IZ_pQGF4N-8eMDInqF7RHQ.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/984/1*IZ_pQGF4N-8eMDInqF7RHQ.png" alt="Image for post"></p>
<p><strong>3. Second Group On the original Key</strong></p>
<p><img src="https://miro.medium.com/max/60/1*vMMG9g7hyxKEVZUdqGCPMQ.png?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/985/1*vMMG9g7hyxKEVZUdqGCPMQ.png" alt="Image for post"></p>
<p>Here we saw how the sum of sum can be used to get the final sum. You can also make use of facts like:</p>
<ul>
<li>min of min is min</li>
<li>max of max is max</li>
<li>sum of count is count</li>
</ul>
<p>You can think about ways in which salting as an idea could be applied to joins too.</p>
<h1 id="Some-More-Tips-and-Tricks"><a href="#Some-More-Tips-and-Tricks" class="headerlink" title="Some More Tips and Tricks"></a>Some More Tips and Tricks</h1><h2 id="Caching"><a href="#Caching" class="headerlink" title="Caching"></a>Caching</h2><p>Spark works on the lazy execution principle. What that means is that nothing really gets executed until you use an action function like the <code>.count()</code> on a dataframe. And if you do a <code>.count</code> function, it generally helps to cache at this step. So I have made it a point to cache() my dataframes whenever I do a <code>.count()</code> operation.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.cache().count()</span><br></pre></td></tr></table></figure>
<h2 id="Save-and-Load-from-an-intermediate-step"><a href="#Save-and-Load-from-an-intermediate-step" class="headerlink" title="Save and Load from an intermediate step"></a>Save and Load from an intermediate step</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.write.parquet(&quot;data/df.parquet&quot;)</span><br><span class="line">df.unpersist()</span><br><span class="line">spark.read.load(&quot;data/df.parquet&quot;)</span><br></pre></td></tr></table></figure>
<p>When you work with Spark you will frequently run with memory and storage issues. While in some cases such issues might be resolved using techniques like broadcasting, salting or cache, sometimes just interrupting the workflow and saving and reloading the whole dataframe at a crucial step has helped me a lot. This helps spark to let go of a lot of memory that gets utilized for storing intermediate shuffle data and unused caches.</p>
<h2 id="Repartitioning"><a href="#Repartitioning" class="headerlink" title="Repartitioning"></a>Repartitioning</h2><p>You might want to repartition your data if you feel your data has been skewed while working with all the transformations and joins. The simplest way to do it is by using:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.repartition(1000)</span><br></pre></td></tr></table></figure>
<p>Sometimes you might also want to repartition by a known scheme as this scheme might be used by a certain join or aggregation operation later on. You can use multiple columns to repartition using:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.repartition(&apos;cola&apos;, &apos;colb&apos;,&apos;colc&apos;,&apos;cold&apos;)</span><br></pre></td></tr></table></figure>
<p>You can get the number of partitions in a data frame using:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.rdd.getNumPartitions()</span><br></pre></td></tr></table></figure>
<p>You can also check out the distribution of records in a partition by using the <code>glom</code> function. This helps in understanding the skew in the data that happens while working with various transformations.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.glom().map(len).collect()</span><br></pre></td></tr></table></figure>
<h2 id="Reading-Parquet-File-in-Local"><a href="#Reading-Parquet-File-in-Local" class="headerlink" title="Reading Parquet File in Local"></a>Reading Parquet File in Local</h2><p>Sometimes you might want to read the parquet files in a system where Spark is not available. In such cases, I normally use the below code:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from glob import glob</span><br><span class="line">def load_df_from_parquet(parquet_directory):</span><br><span class="line">   df = pd.DataFrame()</span><br><span class="line">   for file in glob(f&quot;&#123;parquet_directory&#125;/*&quot;):</span><br><span class="line">      df = pd.concat([df,pd.read_parquet(file)])</span><br><span class="line">   return df</span><br></pre></td></tr></table></figure>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p><img src="https://miro.medium.com/max/60/0*mL0xSG32jEU-gPkb.jpg?q=20" alt="Image for post"></p>
<p><img src="https://miro.medium.com/max/960/0*mL0xSG32jEU-gPkb.jpg" alt="Image for post"></p>
<p>Source: <a href="https://pixabay.com/photos/dawn-graduates-throwing-hats-dusk-1840298/" target="_blank" rel="noopener">Pixabay</a></p>
<p>This was a big post and congratulations on you reaching the end. These are the most common functionalities I end up using in my day to day job.</p>
<p>Hopefully, I’ve covered the Dataframe basics well enough to pique your interest and help you get started with Spark. If you want to learn more about how Spark Started or RDD basics take a look at this <a href="https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a" target="_blank" rel="noopener">post</a></p>
<p><strong><em>You can find all the code at this\</em></strong> <a href="https://github.com/MLWhiz/data_science_blogs/tree/master/sparkdf" target="_blank" rel="noopener"><strong><em>GitHub\</em></strong></a> <strong><em>repository where I keep code for all my posts.\</em></strong></p>
<h2 id="Continue-Learning"><a href="#Continue-Learning" class="headerlink" title="Continue Learning"></a>Continue Learning</h2><p>Also, if you want to learn more about Spark and Spark DataFrames, I would like to call out these excellent courses on <a href="https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=467035.11468293556&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fbig-data-essentials" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a> and <a href="https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=467035.11468293488&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fbig-data-analysis" target="_blank" rel="noopener">Big Data Analysis: Hive, Spark SQL, DataFrames and GraphFrames</a> by Yandex on Coursera.</p>
<p>I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at <a href="https://medium.com/@rahul_agarwal" target="_blank" rel="noopener"><strong>Medium</strong></a> or Subscribe to my <a href="http://eepurl.com/dbQnuX" target="_blank" rel="noopener"><strong>blog</strong></a> to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter <a href="https://twitter.com/MLWhiz" target="_blank" rel="noopener">@mlwhiz</a>.</p>
<p>Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://zhang35.github.io/2020-Spark/pyspark-dataframe.html" data-id="ckhmzdgwk000zzgr3sm7a1fu9" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACKElEQVR42u3aQW7DMAwEwPz/0y7QU4Eizq6YBLA8OhWtK3l8IEiRj0e8jt/VPvn/v44/a35WvTAwMC7LOE7Xs+Pz1813S37/9ENgYGDcgJGEzgksD8Fr+2BgYGA8Oz5J+5IAmpAwMDAwJuVoG4jPkzwMDAyMSRGbH7OWdH6pFsfAwLggY3Ld/+mfP9jfwMDAuAjjKFceLt919R/tj4GBsTUjOaCNdfOBjPzE6FUwMDC2YCQNxbVL/Hy3JJQXA2QYGBjbMeaBtQ21SaJZj2JgYGBszUguv/JXT1oF7ehG3eXAwMDYlNEmZO21Whvc1/6KgYGxK6N9lXaH+SeI9sfAwNiacZ6ctbB8n7ZJELUuMDAwNmUkYbF9sh0Cyz/Qi2ELDAyMGzC+X3y2ITgKuBgYGBsx1i7U8qA8L2KT0zEwMO7AyMe52tGKPKCvnY6BgXEfxrsK1CJEJhlr3ljFwMC4ASNP8taen3+yF8UwBgbG1oy2YTlPGduBs/oSEAMDYzvGZDCrbQa0V2k5GwMD426MtkmwVpS2F21FEYuBgbEdYy1pyy/x26u9NSQGBsbejKNckxK3vbYrCmYMDIytGZOBiTY1fNdgxwiDgYFxWcZaEMyfn49uRAEdAwPjBoy27Fwb+UrOyktZDAwMjHbMoh2kyJsNUZMAAwMDIyhB87C7NsDx4n8xMDBuwFgrYs+PX2t5rrUNMDAw9ma0jYE2WXwMVvKJMTAwtmb8AFdqzFL68LVMAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/pyspark/"><i class="fa fa-tag"></i>pyspark</a><a href="/tags/spark/"><i class="fa fa-tag"></i>spark</a></div><div class="post-nav"><a class="pre" href="/2020-Spark/spark-hdfs.html">Spark操作hdfs</a><a class="next" href="/2020-Spark/spark-scala-wordcount.html">Spark WordCount代码</a></div></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Git学习笔记/">Git学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java-Web/">Java Web</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SQL/">SQL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo个人博客/">hexo个人博客</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/scala/">scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/vba编程/">vba编程</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/vim学习笔记/">vim学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/前端开发/">前端开发</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书笔记/">读书笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/pyspark/" style="font-size: 15px;">pyspark</a> <a href="/tags/高效能-七个习惯/" style="font-size: 15px;">高效能 七个习惯</a> <a href="/tags/bubble-sort/" style="font-size: 15px;">bubble sort</a> <a href="/tags/c/" style="font-size: 15px;">c</a> <a href="/tags/both-sides/" style="font-size: 15px;">both sides</a> <a href="/tags/centos7/" style="font-size: 15px;">centos7</a> <a href="/tags/offline/" style="font-size: 15px;">offline</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/Win7/" style="font-size: 15px;">Win7</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/next/" style="font-size: 15px;">next</a> <a href="/tags/gitpage/" style="font-size: 15px;">gitpage</a> <a href="/tags/PicGo/" style="font-size: 15px;">PicGo</a> <a href="/tags/Typora/" style="font-size: 15px;">Typora</a> <a href="/tags/Github/" style="font-size: 15px;">Github</a> <a href="/tags/Gitee/" style="font-size: 15px;">Gitee</a> <a href="/tags/二叉树/" style="font-size: 15px;">二叉树</a> <a href="/tags/maupassant/" style="font-size: 15px;">maupassant</a> <a href="/tags/sql/" style="font-size: 15px;">sql</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/vim/" style="font-size: 15px;">vim</a> <a href="/tags/ppt/" style="font-size: 15px;">ppt</a> <a href="/tags/excel/" style="font-size: 15px;">excel</a> <a href="/tags/vba/" style="font-size: 15px;">vba</a> <a href="/tags/AngularJS-web/" style="font-size: 15px;">AngularJS web</a> <a href="/tags/freecodecamp/" style="font-size: 15px;">freecodecamp</a> <a href="/tags/gulp/" style="font-size: 15px;">gulp</a> <a href="/tags/bootstrap/" style="font-size: 15px;">bootstrap</a> <a href="/tags/java-web/" style="font-size: 15px;">java web</a> <a href="/tags/量化考评-web-购物车-成长曲线-评语-angular-js-gulp-bower-jade/" style="font-size: 15px;">量化考评 web 购物车 成长曲线 评语 angular.js gulp bower jade</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/gitignore/" style="font-size: 15px;">.gitignore</a> <a href="/tags/github/" style="font-size: 15px;">github</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020-Docker/docker-mysql.html">docker搭建mysql环境</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Python/Flask-SQLAlchemy的增，删，改，查.html">【转】Flask-SQLAlchemy使用方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Python/http.html">Python 使用requests发送Http Get请求</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Postgis/queries.html">Postgis/queries</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Docker/docker-offline-install.html">Centos7 离线安装Docker</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Docker/docker-win7-install.html">Win7安装Docker</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Scala/scala-try-catch.html">scala try-Catch</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Spark/spark-hdfs.html">Spark操作hdfs</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Spark/pyspark-dataframe.html">Spark DataFrame操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Spark/spark-scala-wordcount.html">Spark WordCount代码</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/zhang35" title="github/zhang35" target="_blank">github/zhang35</a><ul></ul><a href="https://blog.csdn.net/zhang35" title="zhang35的博客_CSDN" target="_blank">zhang35的博客_CSDN</a><ul></ul><a href="https://www.jianshu.com/u/6c4f283454b5" title="篮筐轰炸机5号_简书" target="_blank">篮筐轰炸机5号_简书</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">zhang35's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=0.0.0"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>