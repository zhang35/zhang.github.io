<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Spark核心概念RDD | zhang35's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark核心概念RDD</h1><a id="logo" href="/.">zhang35's Blog</a><p class="description">日积月累</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 主页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Spark核心概念RDD</h1><div class="post-meta">2020-09-23<span> | </span><span class="category"><a href="/categories/spark/">spark</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3,920</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 19</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>RDD（Resilient Distributed Datasets）弹性分布式数据集 ，是Spark中最基本的抽象，在 RDD 源码中这样来描述 RDD：</p>
<ul>
<li><p>A list of partitions</p>
</li>
<li><p>A function for computing each split</p>
</li>
<li><p>A list of dependencies on other RDDs</p>
</li>
<li><p>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</p>
</li>
<li><p>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</p>
</li>
</ul>
<p>个人理解，RDD可并行化数据集的抽象，对它的操作能自动分发到集群上处理。</p>
<p>RDD支持两种操作：转换（transformation）从现有的数据集创建一个新的数据集；而动作（actions）在数据集上运行计算后，返回一个值给驱动程序。</p>
<p>经典的WordCount例子，同时包含了actions和transformation，如下图：</p>
<p><img src="https://img-blog.csdnimg.cn/20200924133739758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW5nMzU=,size_16,color_FFFFFF,t_70#pic_center" alt="请添加图片描述"></p>
<p>其中hello.txt如下图：</p>
<p><img src="https://img-blog.csdnimg.cn/20200924133739657.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW5nMzU=,size_16,color_FFFFFF,t_70#pic_center" alt="请添加图片描述"></p>
<p>下面是transformation和actions的常用算子介绍。</p>
<h2 id="一、Transformation"><a href="#一、Transformation" class="headerlink" title="一、Transformation"></a>一、Transformation</h2><p>spark 常用的 Transformation 算子如下表：</p>
<table>
<thead>
<tr>
<th>Transformation 算子</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>map</strong>(<em>func</em>)</td>
<td>对原 RDD 中每个元素运用 <em>func</em> 函数，并生成新的 RDD</td>
</tr>
<tr>
<td><strong>filter</strong>(<em>func</em>)</td>
<td>对原 RDD 中每个元素使用<em>func</em> 函数进行过滤，并生成新的 RDD</td>
</tr>
<tr>
<td><strong>flatMap</strong>(<em>func</em>)</td>
<td>与 map 类似，但是每一个输入的 item 被映射成 0 个或多个输出的 items（ <em>func</em> 返回类型需要为 Seq ）。</td>
</tr>
<tr>
<td><strong>mapPartitions</strong>(<em>func</em>)</td>
<td>与 map 类似，但函数单独在 RDD 的每个分区上运行， <em>func</em>函数的类型为 Iterator<t> =&gt; Iterator<u> ，其中 T 是 RDD 的类型，即 RDD[T]</u></t></td>
</tr>
<tr>
<td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>
<td>与 mapPartitions 类似，但 <em>func</em> 类型为 (Int, Iterator<t>) =&gt; Iterator<u> ，其中第一个参数为分区索引</u></t></td>
</tr>
<tr>
<td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>
<td>数据采样，有三个可选参数：设置是否放回（withReplacement）、采样的百分比（<em>fraction</em>）、随机数生成器的种子（seed）；</td>
</tr>
<tr>
<td><strong>union</strong>(<em>otherDataset</em>)</td>
<td>合并两个 RDD</td>
</tr>
<tr>
<td><strong>intersection</strong>(<em>otherDataset</em>)</td>
<td>求两个 RDD 的交集</td>
</tr>
<tr>
<td><strong>distinct</strong>([<em>numTasks</em>]))</td>
<td>去重</td>
</tr>
<tr>
<td><strong>groupByKey</strong>([<em>numTasks</em>])</td>
<td>按照 key 值进行分区，即在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, Iterable<v>) <strong>Note:</strong> 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 <code>reduceByKey</code> 或 <code>aggregateByKey</code> 性能会更好 <strong>Note:</strong> 默认情况下，并行度取决于父 RDD 的分区数。可以传入 <code>numTasks</code> 参数进行修改。</v></td>
</tr>
<tr>
<td><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td>
<td>按照 key 值进行分组，并对分组后的数据执行归约操作。</td>
</tr>
<tr>
<td><strong>aggregateByKey</strong>(<em>zeroValue</em>,<em>numPartitions</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numTasks</em>])</td>
<td>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 groupByKey 类似，reduce 任务的数量可通过第二个参数进行配置。</td>
</tr>
<tr>
<td><strong>sortByKey</strong>([<em>ascending</em>], [<em>numTasks</em>])</td>
<td>按照 key 进行排序，其中的 key 需要实现 Ordered 特质，即可比较</td>
</tr>
<tr>
<td><strong>join</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>
<td>在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</td>
</tr>
<tr>
<td><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>
<td>在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, (Iterable<v>, Iterable<w>)) tuples 的 dataset。</w></v></td>
</tr>
<tr>
<td><strong>cartesian</strong>(<em>otherDataset</em>)</td>
<td>在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) 类型的 dataset（即笛卡尔积）。</td>
</tr>
<tr>
<td><strong>coalesce</strong>(<em>numPartitions</em>)</td>
<td>将 RDD 中的分区数减少为 numPartitions。</td>
</tr>
<tr>
<td><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td>随机重新调整 RDD 中的数据以创建更多或更少的分区，并在它们之间进行平衡。</td>
</tr>
<tr>
<td><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td>
<td>根据给定的 partitioner（分区器）对 RDD 进行重新分区，并对分区中的数据按照 key 值进行排序。这比调用 <code>repartition</code> 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作所在的机器。</td>
</tr>
</tbody>
</table>
<p>下面分别给出这些算子的基本使用示例：</p>
<h3 id="1-1-map"><a href="#1-1-map" class="headerlink" title="1.1 map"></a>1.1 map</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">sc.parallelize(list).map(_ * <span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果： 10 20 30 （这里为了节省篇幅去掉了换行,后文亦同）</span></span><br></pre></td></tr></table></figure>
<h3 id="1-2-filter"><a href="#1-2-filter" class="headerlink" title="1.2 filter"></a>1.2 filter</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">21</span>)</span><br><span class="line">sc.parallelize(list).filter(_ &gt;= <span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： 10 12 21</span></span><br></pre></td></tr></table></figure>
<h3 id="1-3-flatMap"><a href="#1-3-flatMap" class="headerlink" title="1.3 flatMap"></a>1.3 flatMap</h3><p><code>flatMap(func)</code> 与 <code>map</code> 类似，但每一个输入的 item 会被映射成 0 个或多个输出的 items（ <em>func</em> 返回类型需要为 <code>Seq</code>）。</p>
<p>二者的区别如图：</p>
<h5 id="map"><a href="#map" class="headerlink" title="map"></a>map</h5><p><img src="https://img-blog.csdnimg.cn/20200924133915342.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW5nMzU=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h5 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h5><p><img src="https://img-blog.csdnimg.cn/2020092413392219.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW5nMzU=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val list = List(List(1, 2), List(3), List(), List(4, 5))</span><br><span class="line">sc.parallelize(list).flatMap(_.toList).map(_ * 10).foreach(println)</span><br><span class="line"></span><br><span class="line">// 输出结果 ： 10 20 30 40 50</span><br></pre></td></tr></table></figure>
<p>flatMap 这个算子在日志分析中使用概率非常高，这里进行一下演示：拆分输入的每行数据为单个单词，并赋值为 1，代表出现一次，之后按照单词分组并统计其出现总次数，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = <span class="type">List</span>(<span class="string">"spark flume spark"</span>,</span><br><span class="line">                 <span class="string">"hadoop flume hive"</span>)</span><br><span class="line">sc.parallelize(lines).flatMap(line =&gt; line.split(<span class="string">" "</span>)).</span><br><span class="line">map(word=&gt;(word,<span class="number">1</span>)).reduceByKey(_+_).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出：</span></span><br><span class="line">(spark,<span class="number">2</span>)</span><br><span class="line">(hive,<span class="number">1</span>)</span><br><span class="line">(hadoop,<span class="number">1</span>)</span><br><span class="line">(flume,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-4-mapPartitions"><a href="#1-4-mapPartitions" class="headerlink" title="1.4 mapPartitions"></a>1.4 mapPartitions</h3><p>与 map 类似，但函数单独在 RDD 的每个分区上运行， <em>func</em>函数的类型为 <code>Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;</code> (其中 T 是 RDD 的类型)，即输入和输出都必须是可迭代类型。</p>
<p>map是对rdd中的每一个元素进行操作；</p>
<p>mapPartitions则是对rdd中的每个分区的迭代器进行操作，优点是快，缺点是可能内存溢出（map会自动回收内存）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list, <span class="number">3</span>).mapPartitions(iterator =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">    buffer.append(iterator.next() * <span class="number">100</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br><span class="line"><span class="comment">//输出结果</span></span><br><span class="line"><span class="number">100</span> <span class="number">200</span> <span class="number">300</span> <span class="number">400</span> <span class="number">500</span> <span class="number">600</span></span><br></pre></td></tr></table></figure>
<h3 id="1-5-mapPartitionsWithIndex"><a href="#1-5-mapPartitionsWithIndex" class="headerlink" title="1.5 mapPartitionsWithIndex"></a>1.5 mapPartitionsWithIndex</h3><p>与 mapPartitions 类似，但 <em>func</em> 类型为 <code>(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;</code> ，其中第一个参数为分区索引。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list, <span class="number">3</span>).mapPartitionsWithIndex((index, iterator) =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]</span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">    buffer.append(index + <span class="string">"分区:"</span> + iterator.next() * <span class="number">100</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//完整输出</span></span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">44</span> <span class="type">INFO</span> <span class="type">DAGScheduler</span>: <span class="type">Submitting</span> <span class="number">3</span> missing tasks from <span class="type">ResultStage</span> <span class="number">0</span> (<span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at mapPartitionsWithIndex at <span class="type">App</span>.scala:<span class="number">15</span>)</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">44</span> <span class="type">INFO</span> <span class="type">TaskSchedulerImpl</span>: <span class="type">Adding</span> task set <span class="number">0.0</span> <span class="keyword">with</span> <span class="number">3</span> tasks</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">44</span> <span class="type">INFO</span> <span class="type">TaskSetManager</span>: <span class="type">Starting</span> task <span class="number">0.0</span> in stage <span class="number">0.0</span> (<span class="type">TID</span> <span class="number">0</span>, localhost, partition <span class="number">0</span>, <span class="type">PROCESS_LOCAL</span>, <span class="number">5321</span> bytes)</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">44</span> <span class="type">INFO</span> <span class="type">Executor</span>: <span class="type">Running</span> task <span class="number">0.0</span> in stage <span class="number">0.0</span> (<span class="type">TID</span> <span class="number">0</span>)</span><br><span class="line"><span class="number">0</span>分区:<span class="number">100</span></span><br><span class="line"><span class="number">0</span>分区:<span class="number">200</span></span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">Executor</span>: <span class="type">Finished</span> task <span class="number">0.0</span> in stage <span class="number">0.0</span> (<span class="type">TID</span> <span class="number">0</span>). <span class="number">843</span> bytes result sent to driver</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">TaskSetManager</span>: <span class="type">Starting</span> task <span class="number">1.0</span> in stage <span class="number">0.0</span> (<span class="type">TID</span> <span class="number">1</span>, localhost, partition <span class="number">1</span>, <span class="type">PROCESS_LOCAL</span>, <span class="number">5321</span> bytes)</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">Executor</span>: <span class="type">Running</span> task <span class="number">1.0</span> in stage <span class="number">0.0</span> (<span class="type">TID</span> <span class="number">1</span>)</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">TaskSetManager</span>: <span class="type">Finished</span> task <span class="number">0.0</span> in stage <span class="number">0.0</span> (<span class="type">TID</span> <span class="number">0</span>) in <span class="number">140</span> ms on localhost (<span class="number">1</span>/<span class="number">3</span>)</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">Executor</span>: <span class="type">Finished</span> task <span class="number">1.0</span> in stage <span class="number">0.0</span> (<span class="type">TID</span> <span class="number">1</span>). <span class="number">756</span> bytes result sent to driver</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">TaskSetManager</span>: <span class="type">Starting</span> task <span class="number">2.0</span> in stage <span class="number">0.0</span> (<span class="type">TID</span> <span class="number">2</span>, localhost, partition <span class="number">2</span>, <span class="type">PROCESS_LOCAL</span>, <span class="number">5321</span> bytes)</span><br><span class="line"><span class="number">1</span>分区:<span class="number">300</span></span><br><span class="line"><span class="number">1</span>分区:<span class="number">400</span></span><br><span class="line"><span class="number">2</span>分区:<span class="number">500</span></span><br><span class="line"><span class="number">2</span>分区:<span class="number">600</span></span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">Executor</span>: <span class="type">Running</span> task <span class="number">2.0</span> in stage <span class="number">0.0</span> (<span class="type">TID</span> <span class="number">2</span>)</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">Executor</span>: <span class="type">Finished</span> task <span class="number">2.0</span> in stage <span class="number">0.0</span> (<span class="type">TID</span> <span class="number">2</span>). <span class="number">756</span> bytes result sent to driver</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">TaskSetManager</span>: <span class="type">Finished</span> task <span class="number">1.0</span> in stage <span class="number">0.0</span> (<span class="type">TID</span> <span class="number">1</span>) in <span class="number">26</span> ms on localhost (<span class="number">2</span>/<span class="number">3</span>)</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">TaskSetManager</span>: <span class="type">Finished</span> task <span class="number">2.0</span> in stage <span class="number">0.0</span> (<span class="type">TID</span> <span class="number">2</span>) in <span class="number">20</span> ms on localhost (<span class="number">3</span>/<span class="number">3</span>)</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">DAGScheduler</span>: <span class="type">ResultStage</span> <span class="number">0</span> (foreach at <span class="type">App</span>.scala:<span class="number">21</span>) finished in <span class="number">0.196</span> s</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">TaskSchedulerImpl</span>: <span class="type">Removed</span> <span class="type">TaskSet</span> <span class="number">0.0</span>, whose tasks have all completed, from pool </span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">DAGScheduler</span>: <span class="type">Job</span> <span class="number">0</span> finished: foreach at <span class="type">App</span>.scala:<span class="number">21</span>, took <span class="number">0.427307</span> s</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">SparkContext</span>: <span class="type">Invoking</span> stop() from shutdown hook</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">SparkUI</span>: <span class="type">Stopped</span> <span class="type">Spark</span> web <span class="type">UI</span> at http:<span class="comment">//192.168.15.16:4040</span></span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">MapOutputTrackerMasterEndpoint</span>: <span class="type">MapOutputTrackerMasterEndpoint</span> stopped!</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">MemoryStore</span>: <span class="type">MemoryStore</span> cleared</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">BlockManager</span>: <span class="type">BlockManager</span> stopped</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">BlockManagerMaster</span>: <span class="type">BlockManagerMaster</span> stopped</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">OutputCommitCoordinator</span>$<span class="type">OutputCommitCoordinatorEndpoint</span>: <span class="type">OutputCommitCoordinator</span> stopped!</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">SparkContext</span>: <span class="type">Successfully</span> stopped <span class="type">SparkContext</span></span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">ShutdownHookManager</span>: <span class="type">Shutdown</span> hook called</span><br><span class="line"><span class="number">20</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">17</span>:<span class="number">51</span>:<span class="number">45</span> <span class="type">INFO</span> <span class="type">ShutdownHookManager</span>: <span class="type">Deleting</span> directory <span class="type">C</span>:\<span class="type">Users</span>\<span class="type">Administrator</span>\<span class="type">AppData</span>\<span class="type">Local</span>\<span class="type">Temp</span>\spark<span class="number">-651</span>a1d82-f2c2<span class="number">-4</span>d0e<span class="number">-92</span>de-e838cef3ad3c</span><br></pre></td></tr></table></figure>
<h3 id="1-6-sample"><a href="#1-6-sample" class="headerlink" title="1.6 sample"></a>1.6 sample</h3><p>数据采样。有三个可选参数：设置是否放回 (withReplacement)、采样的百分比 (fraction)、随机数生成器的种子 (seed) 。</p>
<p>其中<code>fraction</code>参数在<code>withReplacement</code>不同时的含义不同：</p>
<ul>
<li>当<code>withReplacement=false</code>时：表示每个元素被抽到的概率，分数一定是[0,1] ；</li>
<li>当<code>withReplacement=true</code>时：表示选择每个元素的期望次数，分数必须大于等于0。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list).sample(withReplacement = <span class="literal">false</span>, fraction = <span class="number">0.5</span>).foreach(println)</span><br><span class="line"><span class="comment">// 输出：2 4 5 6</span></span><br></pre></td></tr></table></figure>
<h3 id="1-7-union"><a href="#1-7-union" class="headerlink" title="1.7 union"></a>1.7 union</h3><p>合并两个 RDD：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val list1 = List(1, 2, 3)</span><br><span class="line">val list2 = List(4, 5, 6)</span><br><span class="line">sc.parallelize(list1).union(sc.parallelize(list2)).foreach(println)</span><br><span class="line">// 输出: 1 2 3 4 5 6</span><br></pre></td></tr></table></figure>
<h3 id="1-8-intersection"><a href="#1-8-intersection" class="headerlink" title="1.8 intersection"></a>1.8 intersection</h3><p>求两个 RDD 的交集：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val list1 = List(1, 2, 3, 4, 5)</span><br><span class="line">val list2 = List(4, 5, 6)</span><br><span class="line">sc.parallelize(list1).intersection(sc.parallelize(list2)).foreach(println)</span><br><span class="line">// 输出:  4 5</span><br></pre></td></tr></table></figure>
<h3 id="1-9-distinct"><a href="#1-9-distinct" class="headerlink" title="1.9 distinct"></a>1.9 distinct</h3><p>去重：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val list = List(1, 2, 2, 4, 4)</span><br><span class="line">sc.parallelize(list).distinct().foreach(println)</span><br><span class="line">// 输出: 4 1 2</span><br></pre></td></tr></table></figure>
<h3 id="1-10-groupByKey"><a href="#1-10-groupByKey" class="headerlink" title="1.10 groupByKey"></a>1.10 groupByKey</h3><p>按照键进行分组：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">2</span>), (<span class="string">"spark"</span>, <span class="number">3</span>), (<span class="string">"spark"</span>, <span class="number">5</span>), (<span class="string">"storm"</span>, <span class="number">6</span>), (<span class="string">"hadoop"</span>, <span class="number">2</span>))</span><br><span class="line">sc.parallelize(list).groupByKey().map(x =&gt; (x._1, x._2.toList)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line">(spark,<span class="type">List</span>(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line">(hadoop,<span class="type">List</span>(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">(storm,<span class="type">List</span>(<span class="number">6</span>))</span><br></pre></td></tr></table></figure>
<p>注：<code>groupByKey</code>返回类型为<code>CompactBuffer</code>（<code>ArrayBuffer</code>的替代选择，占用内存更少），直接打印结果如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).groupByKey.foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line">(spark,<span class="type">CompactBuffer</span>(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line">(hadoop,<span class="type">CompactBuffer</span>(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">(storm,<span class="type">CompactBuffer</span>(<span class="number">6</span>))</span><br></pre></td></tr></table></figure>
<h3 id="1-11-reduceByKey"><a href="#1-11-reduceByKey" class="headerlink" title="1.11 reduceByKey"></a>1.11 reduceByKey</h3><p>按照键进行归约操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">2</span>), (<span class="string">"spark"</span>, <span class="number">3</span>), (<span class="string">"spark"</span>, <span class="number">5</span>), (<span class="string">"storm"</span>, <span class="number">6</span>), (<span class="string">"hadoop"</span>, <span class="number">2</span>))</span><br><span class="line">sc.parallelize(list).reduceByKey(_ + _).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line">(spark,<span class="number">8</span>)</span><br><span class="line">(hadoop,<span class="number">4</span>)</span><br><span class="line">(storm,<span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<p>经典WordCount算子。</p>
<h3 id="1-12-sortBy-amp-sortByKey"><a href="#1-12-sortBy-amp-sortByKey" class="headerlink" title="1.12 sortBy &amp; sortByKey"></a>1.12 sortBy &amp; sortByKey</h3><p>按照键（100、）进行排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">100</span>, <span class="string">"hadoop"</span>), (<span class="number">90</span>, <span class="string">"spark"</span>), (<span class="number">120</span>, <span class="string">"storm"</span>))</span><br><span class="line">sc.parallelize(list01).sortByKey(ascending = <span class="literal">false</span>).foreach(println)</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(<span class="number">120</span>,storm)</span><br><span class="line">(<span class="number">100</span>,hadoop)</span><br><span class="line">(<span class="number">90</span>,spark)</span><br></pre></td></tr></table></figure>
<p>按照指定元素进行排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="string">"hadoop"</span>,<span class="number">100</span>), (<span class="string">"spark"</span>,<span class="number">90</span>), (<span class="string">"storm"</span>,<span class="number">120</span>))</span><br><span class="line">sc.parallelize(list02).sortBy(x=&gt;x._2,ascending=<span class="literal">false</span>).foreach(println)</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(storm,<span class="number">120</span>)</span><br><span class="line">(hadoop,<span class="number">100</span>)</span><br><span class="line">(spark,<span class="number">90</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-13-join"><a href="#1-13-join" class="headerlink" title="1.13 join"></a>1.13 join</h3><p>在一个 (K, V) 和 (K, W) 类型的 Dataset 上调用时，返回一个 (K, (V, W)) 的 Dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"student01"</span>), (<span class="number">2</span>, <span class="string">"student02"</span>), (<span class="number">3</span>, <span class="string">"student03"</span>))</span><br><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"teacher01"</span>), (<span class="number">2</span>, <span class="string">"teacher02"</span>), (<span class="number">3</span>, <span class="string">"teacher03"</span>))</span><br><span class="line">sc.parallelize(list01).join(sc.parallelize(list02)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(<span class="number">1</span>,(student01,teacher01))</span><br><span class="line">(<span class="number">3</span>,(student03,teacher03))</span><br><span class="line">(<span class="number">2</span>,(student02,teacher02))</span><br></pre></td></tr></table></figure>
<h3 id="1-14-cogroup"><a href="#1-14-cogroup" class="headerlink" title="1.14 cogroup"></a>1.14 cogroup</h3><p>在一个 (K, V) 对的 Dataset 上调用时，返回多个类型为 (K, (Iterable<v>, Iterable<w>)) 的元组所组成的 Dataset。</w></v></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"a"</span>),(<span class="number">1</span>, <span class="string">"a"</span>), (<span class="number">2</span>, <span class="string">"b"</span>), (<span class="number">3</span>, <span class="string">"e"</span>))</span><br><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"A"</span>), (<span class="number">2</span>, <span class="string">"B"</span>), (<span class="number">3</span>, <span class="string">"E"</span>))</span><br><span class="line"><span class="keyword">val</span> list03 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"[ab]"</span>), (<span class="number">2</span>, <span class="string">"[bB]"</span>), (<span class="number">3</span>, <span class="string">"eE"</span>),(<span class="number">3</span>, <span class="string">"eE"</span>))</span><br><span class="line">sc.parallelize(list01).cogroup(sc.parallelize(list02),sc.parallelize(list03)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： 同一个 RDD 中的元素先按照 key 进行分组，然后再对不同 RDD 中的元素按照 key 进行分组</span></span><br><span class="line">(<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a, a),<span class="type">CompactBuffer</span>(<span class="type">A</span>),<span class="type">CompactBuffer</span>([ab])))</span><br><span class="line">(<span class="number">3</span>,(<span class="type">CompactBuffer</span>(e),<span class="type">CompactBuffer</span>(<span class="type">E</span>),<span class="type">CompactBuffer</span>(eE, eE)))</span><br><span class="line">(<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="type">B</span>),<span class="type">CompactBuffer</span>([bB])))</span><br></pre></td></tr></table></figure>
<h3 id="1-15-cartesian"><a href="#1-15-cartesian" class="headerlink" title="1.15 cartesian"></a>1.15 cartesian</h3><p>计算笛卡尔积：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="string">"A"</span>, <span class="string">"B"</span>, <span class="string">"C"</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">sc.parallelize(list1).cartesian(sc.parallelize(list2)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出笛卡尔积</span></span><br><span class="line">(<span class="type">A</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="type">A</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="type">A</span>,<span class="number">3</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">3</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-16-aggregateByKey"><a href="#1-16-aggregateByKey" class="headerlink" title="1.16 aggregateByKey"></a>1.16 aggregateByKey</h3><p>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 <code>groupByKey</code> 类似，reduce 任务的数量可通过第二个参数 <code>numPartitions</code> 进行配置。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 为了清晰，以下所有参数均使用具名传参</span></span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">3</span>), (<span class="string">"hadoop"</span>, <span class="number">2</span>), (<span class="string">"spark"</span>, <span class="number">4</span>), (<span class="string">"spark"</span>, <span class="number">3</span>), (<span class="string">"storm"</span>, <span class="number">6</span>), (<span class="string">"storm"</span>, <span class="number">8</span>))</span><br><span class="line">sc.parallelize(list,numSlices = <span class="number">2</span>).aggregateByKey(zeroValue = <span class="number">0</span>,numPartitions = <span class="number">3</span>)(</span><br><span class="line">      seqOp = math.max(_, _),</span><br><span class="line">      combOp = _ + _</span><br><span class="line">    ).collect.foreach(println)</span><br><span class="line"><span class="comment">//输出结果：</span></span><br><span class="line">(hadoop,<span class="number">3</span>)</span><br><span class="line">(storm,<span class="number">8</span>)</span><br><span class="line">(spark,<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<p>这里使用了 <code>numSlices = 2</code> 指定 aggregateByKey 父操作 parallelize 的分区数量为 2，其执行流程如下：</p>
<p><a href="https://github.com/wangzhiwubigdata/God-Of-BigData/blob/master/pictures/spark-aggregateByKey.png" target="_blank" rel="noopener"><img src="https://github.com/wangzhiwubigdata/God-Of-BigData/raw/master/pictures/spark-aggregateByKey.png" alt="img"></a></p>
<p>基于同样的执行流程，如果 <code>numSlices = 1</code>，则意味着只有输入一个分区，则其最后一步 combOp 相当于是无效的，执行结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(hadoop,3)</span><br><span class="line">(storm,8)</span><br><span class="line">(spark,4)</span><br></pre></td></tr></table></figure>
<p>同样的，如果每个单词对一个分区，即 <code>numSlices = 6</code>，此时相当于求和操作，执行结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(hadoop,5)</span><br><span class="line">(storm,14)</span><br><span class="line">(spark,7)</span><br></pre></td></tr></table></figure>
<p><code>aggregateByKey(zeroValue = 0,numPartitions = 3)</code> 的第二个参数 <code>numPartitions</code> 决定的是输出 RDD 的分区数量，想要验证这个问题，可以对上面代码进行改写，使用 <code>getNumPartitions</code> 方法获取分区数量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(list,numSlices = <span class="number">6</span>).aggregateByKey(zeroValue = <span class="number">0</span>,numPartitions = <span class="number">3</span>)(</span><br><span class="line">  seqOp = math.max(_, _),</span><br><span class="line">  combOp = _ + _</span><br><span class="line">).getNumPartitions</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/wangzhiwubigdata/God-Of-BigData/blob/master/pictures/spark-getpartnum.png" target="_blank" rel="noopener"><img src="https://github.com/wangzhiwubigdata/God-Of-BigData/raw/master/pictures/spark-getpartnum.png" alt="img"></a></p>
<h2 id="二、Action"><a href="#二、Action" class="headerlink" title="二、Action"></a>二、Action</h2><p>Spark 常用的 Action 算子如下：</p>
<table>
<thead>
<tr>
<th>Action（动作）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>reduce</strong>(<em>func</em>)</td>
<td>使用函数<em>func</em>执行归约操作</td>
</tr>
<tr>
<td><strong>collect</strong>()</td>
<td>以一个 array 数组的形式返回 dataset 的所有元素，适用于小结果集。</td>
</tr>
<tr>
<td><strong>count</strong>()</td>
<td>返回 dataset 中元素的个数。</td>
</tr>
<tr>
<td><strong>first</strong>()</td>
<td>返回 dataset 中的第一个元素，等价于 take(1)。</td>
</tr>
<tr>
<td><strong>take</strong>(<em>n</em>)</td>
<td>将数据集中的前 <em>n</em> 个元素作为一个 array 数组返回。</td>
</tr>
<tr>
<td><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [<em>seed</em>])</td>
<td>对一个 dataset 进行随机抽样</td>
</tr>
<tr>
<td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。只适用于小结果集，因为所有数据都会被加载到驱动程序的内存中进行排序。</td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td>将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。</td>
</tr>
<tr>
<td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td>
<td>将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。该操作要求 RDD 中的元素需要实现 Hadoop 的 Writable 接口。对于 Scala 语言而言，它可以将 Spark 中的基本数据类型自动隐式转换为对应 Writable 类型。(目前仅支持 Java and Scala)</td>
</tr>
<tr>
<td><strong>saveAsObjectFile</strong>(<em>path</em>)</td>
<td>使用 Java 序列化后存储，可以使用 <code>SparkContext.objectFile()</code> 进行加载。(目前仅支持 Java and Scala)</td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>计算每个键出现的次数。</td>
</tr>
<tr>
<td><strong>foreach</strong>(<em>func</em>)</td>
<td>遍历 RDD 中每个元素，并对其执行<em>fun</em>函数</td>
</tr>
</tbody>
</table>
<h3 id="2-1-reduce"><a href="#2-1-reduce" class="headerlink" title="2.1 reduce"></a>2.1 reduce</h3><p>使用函数<em>func</em>执行归约操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">sc.parallelize(list).reduce((x, y) =&gt; x + y)</span><br><span class="line">sc.parallelize(list).reduce(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出 15</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-takeOrdered"><a href="#2-2-takeOrdered" class="headerlink" title="2.2 takeOrdered"></a>2.2 takeOrdered</h3><p>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。需要注意的是 <code>takeOrdered</code> 使用隐式参数进行隐式转换，以下为其源码。所以在使用自定义排序时，需要继承 <code>Ordering[T]</code> 实现自定义比较器，然后将其作为隐式参数引入。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  .........</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>自定义规则排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 继承 Ordering[T],实现自定义比较器，按照 value 值的长度进行排序</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[(<span class="type">Int</span>, <span class="type">String</span>)] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: (<span class="type">Int</span>, <span class="type">String</span>), y: (<span class="type">Int</span>, <span class="type">String</span>)): <span class="type">Int</span></span><br><span class="line">    = <span class="keyword">if</span> (x._2.length &gt; y._2.length) <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"hadoop"</span>), (<span class="number">1</span>, <span class="string">"storm"</span>), (<span class="number">1</span>, <span class="string">"azkaban"</span>), (<span class="number">1</span>, <span class="string">"hive"</span>))</span><br><span class="line"><span class="comment">//  引入隐式默认值</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> implicitOrdering = <span class="keyword">new</span> <span class="type">CustomOrdering</span></span><br><span class="line">sc.parallelize(list).takeOrdered(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： Array((1,hive), (1,storm), (1,hadoop), (1,azkaban)</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-countByKey"><a href="#2-3-countByKey" class="headerlink" title="2.3 countByKey"></a>2.3 countByKey</h3><p>计算每个键出现的次数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"storm"</span>, <span class="number">3</span>), (<span class="string">"storm"</span>, <span class="number">3</span>), (<span class="string">"azkaban"</span>, <span class="number">1</span>))</span><br><span class="line">sc.parallelize(list).countByKey()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： Map(hadoop -&gt; 2, storm -&gt; 2, azkaban -&gt; 1)</span></span><br></pre></td></tr></table></figure>
<h3 id="2-4-saveAsTextFile"><a href="#2-4-saveAsTextFile" class="headerlink" title="2.4 saveAsTextFile"></a>2.4 saveAsTextFile</h3><p>将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"storm"</span>, <span class="number">3</span>), (<span class="string">"storm"</span>, <span class="number">3</span>), (<span class="string">"azkaban"</span>, <span class="number">1</span>))</span><br><span class="line">sc.parallelize(list).saveAsTextFile(<span class="string">"/usr/file/temp"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-programming-guide" target="_blank" rel="noopener">RDD Programming Guide</a></p>
<p><a href="https://github.com/wangzhiwubigdata/God-Of-BigData/blob/master/大数据框架学习/Spark_Transformation和Action算子.md" target="_blank" rel="noopener">RDD 常用算子详解</a></p>
<p><a href="https://www.cnblogs.com/qingyunzong/p/8899715.html" target="_blank" rel="noopener">Spark学习之路（三）Spark之RDD - 扎心了，老铁- 博客园</a></p>
<p><a href="https://juejin.im/post/6844903846066520071" target="_blank" rel="noopener">到处是map、flatMap，啥意思？ - 掘金</a></p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://zhang35.github.io/2020-Spark/spark-rdd.html" data-id="ckfkokgkq0026bwr3hxdyw4wy" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACG0lEQVR42u3aQW4DIQwF0N7/0um2UhXyjZlIwGNVTScDj4UFtn9+4vH6M8bv/H9//KvxdxYPDAyMbRmv4Rgv69074yfJGvK1YWBg3MNIAmh1ucn0HTYGBgbGmDFebs7L58LAwMCYYyRX3A0CLgYGxiaMJHQm7yeJtlVzYWBg3MOoJuu/+fcj9Q0MDIytGHOhs1M8eGRVGBgYRzPyaZKjXt6W0WdjYGBg5Kn/anJtbrOSJxgYGKcyOovLw2KnPJAUJDAwMM5mrDrkJYm5pEg5V1rAwMA4m5F8rh+CO81h443AwMC4h5En+nNeP+AW7qcYGBhHM/LLYZK476Th8kTbh6QbBgbGBYwnAm6+BfkGfShhYmBgXMCYC8FzDRnV8P32VxgYGEcz8tRYkizrp+Gqh04MDIyzGXn5ML+Crt2CKOhjYGBcw/jOslYl2sqnSAwMjCMYSXFx7qJbbbNY0DOCgYFxEKPaMJGn0iaT+8XQXw7EGBgY2zKqqbG5ZrJqubRcEMXAwLiA8URhIE/VVcP35FUWAwPjCEbnmppcSjv11Q/JOwwMjEMZr+LIk/79C2phazAwMI5mdIJd9UP9oyEGBsbNjDyAVhs1kv92ipQYGBi3MaqFybzNK3/SaWXDwMDAyJc7dwKd2w4MDAyMtffgagovKbIuOxpiYGBswqgm+uemnGs7S5pCMDAwzmZ0jm5zTxLS2rIEBgbGtoxfndjRP0VqzmEAAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/spark/"><i class="fa fa-tag"></i>spark</a></div><div class="post-nav"><a class="pre" href="/2020-Python/python-grammar.html">python-grammar</a><a class="next" href="/2020-LeetCode/leetcode-617.html">leetcode[617] Merge Two Binary Trees Python3实现</a></div></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Git学习笔记/">Git学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java-Web/">Java Web</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SQL/">SQL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo个人博客/">hexo个人博客</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/scala/">scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/vba编程/">vba编程</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/vim学习笔记/">vim学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/前端开发/">前端开发</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书笔记/">读书笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/AngularJS-web/" style="font-size: 15px;">AngularJS web</a> <a href="/tags/高效能-七个习惯/" style="font-size: 15px;">高效能 七个习惯</a> <a href="/tags/freecodecamp/" style="font-size: 15px;">freecodecamp</a> <a href="/tags/bubble-sort/" style="font-size: 15px;">bubble sort</a> <a href="/tags/c/" style="font-size: 15px;">c</a> <a href="/tags/both-sides/" style="font-size: 15px;">both sides</a> <a href="/tags/bootstrap/" style="font-size: 15px;">bootstrap</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/gitignore/" style="font-size: 15px;">.gitignore</a> <a href="/tags/github/" style="font-size: 15px;">github</a> <a href="/tags/gulp/" style="font-size: 15px;">gulp</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/next/" style="font-size: 15px;">next</a> <a href="/tags/gitpage/" style="font-size: 15px;">gitpage</a> <a href="/tags/java-web/" style="font-size: 15px;">java web</a> <a href="/tags/二叉树/" style="font-size: 15px;">二叉树</a> <a href="/tags/ppt/" style="font-size: 15px;">ppt</a> <a href="/tags/excel/" style="font-size: 15px;">excel</a> <a href="/tags/vba/" style="font-size: 15px;">vba</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/maupassant/" style="font-size: 15px;">maupassant</a> <a href="/tags/量化考评-web-购物车-成长曲线-评语-angular-js-gulp-bower-jade/" style="font-size: 15px;">量化考评 web 购物车 成长曲线 评语 angular.js gulp bower jade</a> <a href="/tags/vim/" style="font-size: 15px;">vim</a> <a href="/tags/Leetcode-二叉树/" style="font-size: 15px;">Leetcode 二叉树</a> <a href="/tags/PicGo/" style="font-size: 15px;">PicGo</a> <a href="/tags/Typora/" style="font-size: 15px;">Typora</a> <a href="/tags/Github/" style="font-size: 15px;">Github</a> <a href="/tags/sql/" style="font-size: 15px;">sql</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020-Scala/scala-try-catch.html">scala try-Catch</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Spark/spark-hdfs.html">Spark操作hdfs</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Spark/scala-json.html">scala解析json</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Scala/scala-json.html">scala解析json</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Spark/spark-sql.html">Spark读写数据</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-LeetCode/leetcode-145.html">leetcode[145]Binary Tree Postorder Traversal Python3实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-SQL/sql-grammar.html">sql-grammar</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-Hexo/picGo-github-image-bed.html">PicGo + Typora + Github 搭建博客图床</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-LeetCode/leetcode-235.html">leetcode[235]Lowest Common Ancestor of a Binary Search Tree Python3实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2020-LeetCode/leetcode-106.html">leetcode[106]从中序与后序遍历序列构造二叉树 Python3实现</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/zhang35" title="github/zhang35" target="_blank">github/zhang35</a><ul></ul><a href="https://blog.csdn.net/zhang35" title="zhang35的博客_CSDN" target="_blank">zhang35的博客_CSDN</a><ul></ul><a href="https://www.jianshu.com/u/6c4f283454b5" title="篮筐轰炸机5号_简书" target="_blank">篮筐轰炸机5号_简书</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">zhang35's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=0.0.0"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>